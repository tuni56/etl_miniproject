# etl_miniproject con python y AWS S3

## ðŸŽ¯ Objetivo
Crear un pipeline ETL (Extract, Transform, Load) en AWS que:

Extraiga datos desde un archivo CSV en un bucket de S3.
Transforme los datos con Pandas en Python (limpieza, normalizaciÃ³n, agregaciones).
Cargue los datos procesados en otro bucket de S3 o en una base de datos como Amazon RDS.

## ðŸ›  TecnologÃ­as
âœ… Python (Pandas, Boto3)
âœ… AWS S3 (para almacenamiento de datos)
âœ… AWS IAM (gestiÃ³n de permisos)
âœ… Docker (opcional, para ejecutar en un contenedor)
âœ… Jupyter Notebook o Script Python (para desarrollo y pruebas)

## ðŸ“Œ Pasos del Proyecto
### ðŸ”¹ 1. ConfiguraciÃ³n de AWS S3 e IAM
Crear un bucket de S3 llamado etl-source-data para almacenar los datos sin procesar.
Crear otro bucket etl-processed-data para los datos transformados.
Crear un usuario IAM con permisos para S3 (s3:PutObject, s3:GetObject, etc.).
Configurar credenciales en ~/.aws/credentials o usar variables de entorno.
### ðŸ”¹ 2. Subir un Dataset a S3
Descargar un dataset pÃºblico (por ejemplo, sobre ventas, clima, etc.).
Subirlo manualmente a etl-source-data o usar un script Python con Boto3.
### ðŸ”¹ 3. Desarrollo del Script ETL con Python
El script debe:

Extraer el archivo CSV desde S3.
Transformar los datos (limpieza, conversiÃ³n de tipos, agregaciones).
Cargar los datos procesados en otro bucket o en RDS.
### ðŸ”¹ 4. AutomatizaciÃ³n con AWS Lambda (Opcional)
Subir el script a AWS Lambda.
Configurar un trigger para ejecutarlo cada vez que se suba un nuevo archivo a S3.
